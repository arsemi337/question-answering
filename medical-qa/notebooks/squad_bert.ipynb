{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-20T18:00:57.914863900Z",
     "start_time": "2023-10-20T18:00:50.756589500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b7529904f21909",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T18:00:58.993683500Z",
     "start_time": "2023-10-20T18:00:57.916871700Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/datasets/squad/train.csv')\n",
    "df_val = pd.read_csv('../data/datasets/squad/dev.csv')\n",
    "df_test = pd.read_csv('../data/datasets/squad/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db291e0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T18:01:01.207743400Z",
     "start_time": "2023-10-20T18:00:58.992682800Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67e3380",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T18:01:01.377245200Z",
     "start_time": "2023-10-20T18:01:01.207743400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.Dataset.from_pandas(df_train)\n",
    "val_dataset = datasets.Dataset.from_pandas(df_val)\n",
    "test_dataset = datasets.Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df2e95fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T18:01:01.392756200Z",
     "start_time": "2023-10-20T18:01:01.380244400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['Unnamed: 0', 'context', 'question', 'answers', '__index_level_0__'],\n    num_rows: 68718\n})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07683a00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T18:01:01.436255400Z",
     "start_time": "2023-10-20T18:01:01.393756100Z"
    }
   },
   "outputs": [],
   "source": [
    "def firstly_tokenize(row):\n",
    "    return tokenizer(\n",
    "        row[\"question\"],\n",
    "        row[\"context\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d177b07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T18:01:47.932759800Z",
     "start_time": "2023-10-20T18:01:26.912181700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/68718 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "488fd19e49c440b5ad8b6f13ed1ae03a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68718\n"
     ]
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/68718 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80a3f65c2b2d438f90ce091d0aa3e744"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67966\n"
     ]
    }
   ],
   "source": [
    "max_length = 384\n",
    "\n",
    "tokenized_datasets = train_dataset.map(firstly_tokenize, batched=True)\n",
    "print(len(tokenized_datasets))\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example: len(example['input_ids']) <= max_length)\n",
    "print(len(tokenized_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2912c146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T18:15:39.660644400Z",
     "start_time": "2023-10-20T18:13:01.976753Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object <genexpr> at 0x0000016F85993970> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "indices_to_remove = []\n",
    "\n",
    "for i, row in enumerate(train_dataset):\n",
    "    tokenized_row = firstly_tokenize(row)\n",
    "    if len(tokenized_row[\"input_ids\"]) > max_length:\n",
    "        indices_to_remove.append(i)\n",
    "\n",
    "filtered_dataset = train_dataset.select(\n",
    "    i for i in range(len(train_dataset))\n",
    "    if i not in set(indices_to_remove)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67966\n"
     ]
    }
   ],
   "source": [
    "print(len(indices_to_remove))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T18:15:39.676704300Z",
     "start_time": "2023-10-20T18:15:39.664682600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752\n"
     ]
    }
   ],
   "source": [
    "print(68718 - 67966)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-20T18:15:39.721591400Z",
     "start_time": "2023-10-20T18:15:39.678706500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1aebcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(tokenized_datasets[\"input_ids\"], key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f39e322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'context', 'question', 'answers', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 67966\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea647040",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2283b5f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32me:\\STUDIA\\IPS\\question-answering\\medical-qa\\notebooks\\squad_bert.ipynb Cell 7\u001B[0m line \u001B[0;36m1\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mmap_correctly_predicted\u001B[39m(row):\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mint\u001B[39m(row[\u001B[39m'\u001B[39m\u001B[39mlabel\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m==\u001B[39m row[\u001B[39m'\u001B[39m\u001B[39mprediction\u001B[39m\u001B[39m'\u001B[39m])\n\u001B[1;32m---> <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001B[0m test_df[\u001B[39m'\u001B[39m\u001B[39mtoken_count\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m test_df\u001B[39m.\u001B[39mapply(\u001B[39mlambda\u001B[39;00m row: map_token_counts(row), axis\u001B[39m=\u001B[39m\u001B[39m1\u001B[39m)\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001B[0m test_df[\u001B[39m'\u001B[39m\u001B[39mcount_belonging\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m test_df\u001B[39m.\u001B[39mapply(\u001B[39mlambda\u001B[39;00m row: map_count_belonging(row, divider\u001B[39m=\u001B[39m\u001B[39m6\u001B[39m), axis\u001B[39m=\u001B[39m\u001B[39m1\u001B[39m)\n\u001B[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001B[0m test_df[\u001B[39m'\u001B[39m\u001B[39mcorrectly_predicted\u001B[39m\u001B[39m'\u001B[39m] \u001B[39m=\u001B[39m test_df\u001B[39m.\u001B[39mapply(\u001B[39mlambda\u001B[39;00m row: map_correctly_predicted(row), axis\u001B[39m=\u001B[39m\u001B[39m1\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "def map_word_counts(row):\n",
    "    return len(row['text'].split())\n",
    "\n",
    "def map_count_belonging(row, divider):\n",
    "    int_division_result = int(row['token_count'] / divider) + 1\n",
    "    lower_boundary = divider * int_division_result - divider\n",
    "    upper_boundary = divider * int_division_result - 1\n",
    "    return f\"{lower_boundary}-{upper_boundary}\"\n",
    "\n",
    "def map_correctly_predicted(row):\n",
    "    return int(row['label'] == row['prediction'])\n",
    "\n",
    "\n",
    "test_df['token_count'] = test_df.apply(lambda row: map_token_counts(row), axis=1)\n",
    "test_df['count_belonging'] = test_df.apply(lambda row: map_count_belonging(row, divider=6), axis=1)\n",
    "test_df['correctly_predicted'] = test_df.apply(lambda row: map_correctly_predicted(row), axis=1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f25dcf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = train_dataset[0][\"context\"]\n",
    "question = train_dataset[0][\"question\"]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=384,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa426811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] along with the united democratic party, what party currently rules the marshall islands? [SEP] legislative power lies with the nitijela. the upper house of parliament, called the council of iroij, is an advisory body comprising twelve tribal chiefs. the executive branch consists of the president and the presidential cabinet, which consists of ten ministers appointed by the president with the approval of the nitijela. the twenty - four electoral districts into which the country is divided correspond to the inhabited islands and atolls. there are currently four political parties in the marshall islands : aelon kein ad ( aka ), united people's party ( upp ), kien eo am ( kea ) and united democratic party ( udp ). rule is shared by the aka and the udp. the following senators are in the legislative body : [SEP]\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs.input_ids[0])\n",
    "# len(inputs.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2364113d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87df8da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (3, 7),\n",
       " (8, 11),\n",
       " (12, 15),\n",
       " (16, 22),\n",
       " (23, 27),\n",
       " (28, 37),\n",
       " (38, 44),\n",
       " (45, 47),\n",
       " (48, 52),\n",
       " (53, 55),\n",
       " (56, 59),\n",
       " (59, 63),\n",
       " (64, 70),\n",
       " (70, 71),\n",
       " (0, 0),\n",
       " (0, 13),\n",
       " (13, 15),\n",
       " (15, 16),\n",
       " (17, 20),\n",
       " (21, 27),\n",
       " (28, 31),\n",
       " (32, 33),\n",
       " (34, 42),\n",
       " (43, 52),\n",
       " (52, 53),\n",
       " (54, 56),\n",
       " (56, 58),\n",
       " (59, 62),\n",
       " (63, 67),\n",
       " (68, 76),\n",
       " (76, 77),\n",
       " (77, 78),\n",
       " (79, 83),\n",
       " (84, 88),\n",
       " (89, 91),\n",
       " (92, 93),\n",
       " (94, 100),\n",
       " (101, 107),\n",
       " (108, 110),\n",
       " (111, 114),\n",
       " (115, 121),\n",
       " (122, 126),\n",
       " (126, 127),\n",
       " (128, 139),\n",
       " (140, 142),\n",
       " (143, 148),\n",
       " (149, 151),\n",
       " (152, 155),\n",
       " (156, 160),\n",
       " (161, 169),\n",
       " (170, 173),\n",
       " (174, 180),\n",
       " (181, 183),\n",
       " (183, 184),\n",
       " (185, 187),\n",
       " (188, 189),\n",
       " (190, 196),\n",
       " (197, 203),\n",
       " (204, 206),\n",
       " (207, 213),\n",
       " (214, 218),\n",
       " (219, 223),\n",
       " (224, 226),\n",
       " (226, 229),\n",
       " (229, 232),\n",
       " (233, 237),\n",
       " (238, 241),\n",
       " (242, 248),\n",
       " (249, 250),\n",
       " (250, 251),\n",
       " (251, 254),\n",
       " (254, 256),\n",
       " (257, 259),\n",
       " (260, 262),\n",
       " (263, 264),\n",
       " (264, 265),\n",
       " (265, 268),\n",
       " (268, 269),\n",
       " (269, 270),\n",
       " (271, 275),\n",
       " (276, 278),\n",
       " (279, 282),\n",
       " (283, 287),\n",
       " (288, 296),\n",
       " (297, 299),\n",
       " (300, 303),\n",
       " (304, 312),\n",
       " (313, 315),\n",
       " (316, 319),\n",
       " (320, 326),\n",
       " (327, 332),\n",
       " (332, 333),\n",
       " (334, 345),\n",
       " (346, 352),\n",
       " (353, 356),\n",
       " (357, 358),\n",
       " (358, 361),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"offset_mapping\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44f85631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4 examples gave 19 features.\n",
      "Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][2:6][\"question\"],\n",
    "    raw_datasets[\"train\"][2:6][\"context\"],\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07625613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],\n",
       " [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = raw_datasets[\"train\"][2:6][\"answers\"]\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
    "    answer = answers[sample_idx]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label is (0, 0)\n",
    "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83d9c234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: the Main Building, labels give: the Main Building\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "start = start_positions[idx]\n",
    "end = end_positions[idx]\n",
    "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
    "\n",
    "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26edd6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee20ccb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83a3bf887a24ab09a172428fc254d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(87599, 88729)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29b8f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ba32ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453592238bd94ed099bb694935448018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10570, 10822)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7e3369a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " [0, 5],\n",
       " [6, 10],\n",
       " [11, 13],\n",
       " [14, 17],\n",
       " [18, 20],\n",
       " [21, 29],\n",
       " [30, 38],\n",
       " [39, 43],\n",
       " [44, 46],\n",
       " [47, 56],\n",
       " [57, 60],\n",
       " [61, 69],\n",
       " [70, 72],\n",
       " [73, 76],\n",
       " [77, 85],\n",
       " [86, 94],\n",
       " [95, 101],\n",
       " [102, 103],\n",
       " [103, 106],\n",
       " [106, 107],\n",
       " [108, 111],\n",
       " [112, 115],\n",
       " [116, 120],\n",
       " [121, 127],\n",
       " [127, 128],\n",
       " [129, 132],\n",
       " [133, 141],\n",
       " [142, 150],\n",
       " [151, 161],\n",
       " [162, 163],\n",
       " [163, 166],\n",
       " [166, 167],\n",
       " [168, 176],\n",
       " [177, 183],\n",
       " [184, 191],\n",
       " [192, 200],\n",
       " [201, 204],\n",
       " [205, 213],\n",
       " [214, 222],\n",
       " [223, 233],\n",
       " [234, 235],\n",
       " [235, 238],\n",
       " [238, 239],\n",
       " [240, 248],\n",
       " [249, 257],\n",
       " [258, 266],\n",
       " [267, 269],\n",
       " [269, 270],\n",
       " [270, 272],\n",
       " [273, 275],\n",
       " [276, 280],\n",
       " [281, 286],\n",
       " [287, 292],\n",
       " [293, 298],\n",
       " [299, 303],\n",
       " [304, 309],\n",
       " [309, 310],\n",
       " [311, 314],\n",
       " [315, 319],\n",
       " [320, 323],\n",
       " [324, 330],\n",
       " [331, 333],\n",
       " [334, 342],\n",
       " [343, 344],\n",
       " [344, 345],\n",
       " [346, 350],\n",
       " [350, 351],\n",
       " [352, 354],\n",
       " [355, 359],\n",
       " [359, 360],\n",
       " [360, 361],\n",
       " [362, 369],\n",
       " [370, 372],\n",
       " [373, 376],\n",
       " [377, 380],\n",
       " [381, 390],\n",
       " [391, 394],\n",
       " [395, 399],\n",
       " [400, 402],\n",
       " [403, 408],\n",
       " [409, 414],\n",
       " [414, 415],\n",
       " [416, 426],\n",
       " [426, 427],\n",
       " [428, 430],\n",
       " [431, 435],\n",
       " [436, 439],\n",
       " [440, 443],\n",
       " [444, 448],\n",
       " [449, 454],\n",
       " [455, 459],\n",
       " [459, 460],\n",
       " [461, 464],\n",
       " [465, 471],\n",
       " [472, 482],\n",
       " [483, 486],\n",
       " [487, 488],\n",
       " [488, 494],\n",
       " [495, 506],\n",
       " [506, 507],\n",
       " [508, 512],\n",
       " [513, 520],\n",
       " [521, 525],\n",
       " [525, 526],\n",
       " [526, 532],\n",
       " [533, 544],\n",
       " [544, 545],\n",
       " [546, 548],\n",
       " [549, 553],\n",
       " [554, 556],\n",
       " [557, 568],\n",
       " [569, 571],\n",
       " [571, 573],\n",
       " [573, 579],\n",
       " [580, 583],\n",
       " [584, 593],\n",
       " [594, 596],\n",
       " [597, 603],\n",
       " [604, 608],\n",
       " [609, 614],\n",
       " [615, 619],\n",
       " [620, 624],\n",
       " [625, 629],\n",
       " [630, 635],\n",
       " [636, 637],\n",
       " [637, 640],\n",
       " [640, 644],\n",
       " [645, 646],\n",
       " [646, 651],\n",
       " [652, 657],\n",
       " [658, 661],\n",
       " [662, 666],\n",
       " [667, 672],\n",
       " [673, 677],\n",
       " [678, 682],\n",
       " [683, 688],\n",
       " [689, 691],\n",
       " [692, 693],\n",
       " [693, 698],\n",
       " [699, 703],\n",
       " [704, 705],\n",
       " [705, 706],\n",
       " [706, 707],\n",
       " [707, 708],\n",
       " [709, 711],\n",
       " [712, 716],\n",
       " [717, 720],\n",
       " [721, 725],\n",
       " [726, 731],\n",
       " [732, 743],\n",
       " [744, 751],\n",
       " [752, 755],\n",
       " [756, 762],\n",
       " [763, 764],\n",
       " [764, 767],\n",
       " [767, 771],\n",
       " [772, 774],\n",
       " [774, 775],\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset[\"offset_mapping\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6eabf099",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = raw_datasets[\"train\"][2:6][\"answers\"]\n",
    "start_positions = []\n",
    "end_positions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1d12478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': ['the Main Building'], 'answer_start': [279]},\n",
       " {'text': ['a Marian place of prayer and reflection'], 'answer_start': [381]},\n",
       " {'text': ['a golden statue of the Virgin Mary'], 'answer_start': [92]},\n",
       " {'text': ['September 1876'], 'answer_start': [248]}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ae1c389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661180',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?',\n",
       " 'answers': {'text': ['the Main Building'], 'answer_start': [279]}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c22c873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.S. Customs and Border Protection only carries immigration (but not customs) functions. Since Guam is under federal immigration jurisdiction, passengers arriving directly from the United States skip immigration and proceed directly to Guam Customs and Quarantine.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"Guam is served by the Antonio B. Won Pat International Airport, which is a hub for United Airlines. The island is outside the United States customs zone so Guam is responsible for establishing and operating its own customs and quarantine agency and jurisdiction. Therefore, the U.S. Customs and Border Protection only carries immigration (but not customs) functions. Since Guam is under federal immigration jurisdiction, passengers arriving directly from the United States skip immigration and proceed directly to Guam Customs and Quarantine.\"\n",
    "a[279:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46dabf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 3),\n",
       " (4, 12),\n",
       " (13, 15),\n",
       " (16, 19),\n",
       " (20, 26),\n",
       " (27, 32),\n",
       " (33, 35),\n",
       " (36, 41),\n",
       " (42, 46),\n",
       " (47, 49),\n",
       " (50, 56),\n",
       " (57, 59),\n",
       " (60, 65),\n",
       " (66, 75),\n",
       " (75, 76),\n",
       " (0, 0),\n",
       " (0, 13),\n",
       " (13, 15),\n",
       " (15, 16),\n",
       " (17, 20),\n",
       " (21, 27),\n",
       " (28, 31),\n",
       " (32, 33),\n",
       " (34, 42),\n",
       " (43, 52),\n",
       " (52, 53),\n",
       " (54, 58),\n",
       " (59, 62),\n",
       " (63, 67),\n",
       " (68, 76),\n",
       " (76, 77),\n",
       " (77, 78),\n",
       " (79, 83),\n",
       " (84, 88),\n",
       " (89, 91),\n",
       " (92, 93),\n",
       " (94, 100),\n",
       " (101, 107),\n",
       " (108, 110),\n",
       " (111, 114),\n",
       " (115, 121),\n",
       " (122, 126),\n",
       " (126, 127),\n",
       " (128, 139),\n",
       " (140, 142),\n",
       " (143, 148),\n",
       " (149, 151),\n",
       " (152, 155),\n",
       " (156, 160),\n",
       " (161, 169),\n",
       " (170, 173),\n",
       " (174, 180),\n",
       " (181, 183),\n",
       " (183, 184),\n",
       " (185, 187),\n",
       " (188, 189),\n",
       " (190, 196),\n",
       " (197, 203),\n",
       " (204, 206),\n",
       " (207, 213),\n",
       " (214, 218),\n",
       " (219, 223),\n",
       " (224, 226),\n",
       " (226, 229),\n",
       " (229, 232),\n",
       " (233, 237),\n",
       " (238, 241),\n",
       " (242, 248),\n",
       " (249, 250),\n",
       " (250, 252),\n",
       " (252, 254),\n",
       " (254, 256),\n",
       " (257, 259),\n",
       " (260, 262),\n",
       " (263, 265),\n",
       " (265, 268),\n",
       " (268, 269),\n",
       " (269, 270),\n",
       " (271, 275),\n",
       " (276, 278),\n",
       " (279, 282),\n",
       " (283, 287),\n",
       " (288, 296),\n",
       " (297, 299),\n",
       " (300, 303),\n",
       " (304, 312),\n",
       " (313, 315),\n",
       " (316, 319),\n",
       " (320, 326),\n",
       " (327, 332),\n",
       " (332, 333),\n",
       " (334, 345),\n",
       " (346, 352),\n",
       " (353, 356),\n",
       " (357, 365),\n",
       " (366, 368),\n",
       " (369, 372),\n",
       " (373, 375),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[\"offset_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "50b11d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " None,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " None]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][0]\n",
    "answer = answers[sample_idx]\n",
    "start_char = answer[\"answer_start\"][0]\n",
    "end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "sequence_ids = inputs.sequence_ids(0)\n",
    "sequence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a572a716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['the Main Building'], 'answer_start': [279]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ace20ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "while sequence_ids[idx] != 1:\n",
    "    idx += 1\n",
    "context_start = idx\n",
    "while sequence_ids[idx] == 1:\n",
    "    idx += 1\n",
    "context_end = idx - 1\n",
    "context_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33d1e8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"offset_mapping\"][2][17][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4a627f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de8911c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ebad9a80814db69b18f87e47e31a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(87599, 88524)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cf0c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "825abd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0740be383a43559c4b1c27daebe7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10570, 10784)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9a8998c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abdf138ecfb47ec87168c90f26ba7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Artur\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ae1f25457c4072b71c1dbf28abf2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()lve/main/config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882c19df042b4ff0a116dd1eaec96357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193fcf8c1fc042488ffda9303cf1228f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc84482169cc4d21a14ef194a615c2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "small_eval_set = raw_datasets[\"validation\"].select(range(100))\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "eval_set = small_eval_set.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b83abc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "368e6c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc02501f6aa4995bd9e5db4e9dc5403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "eval_set_for_model.set_format(\"numpy\")\n",
    "\n",
    "batch = {k: eval_set_for_model[k] for k in eval_set_for_model.column_names}\n",
    "trained_model = TFAutoModelForQuestionAnswering.from_pretrained(trained_checkpoint)\n",
    "\n",
    "outputs = trained_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7d3c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits.numpy()\n",
    "end_logits = outputs.end_logits.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4744d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "example_to_features = collections.defaultdict(list)\n",
    "for idx, feature in enumerate(eval_set):\n",
    "    example_to_features[feature[\"example_id\"]].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01471346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "for example in small_eval_set:\n",
    "    example_id = example[\"id\"]\n",
    "    context = example[\"context\"]\n",
    "    answers = []\n",
    "\n",
    "    for feature_index in example_to_features[example_id]:\n",
    "        start_logit = start_logits[feature_index]\n",
    "        end_logit = end_logits[feature_index]\n",
    "        offsets = eval_set[\"offset_mapping\"][feature_index]\n",
    "\n",
    "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # Skip answers that are not fully in the context\n",
    "                if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                    continue\n",
    "                # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                if (\n",
    "                    end_index < start_index\n",
    "                    or end_index - start_index + 1 > max_answer_length\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                answers.append(\n",
    "                    {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "affdf93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f76eb9478a64bc5990f8d10be03d8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf44026726c4c7b833fd30d092512fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e69edb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "theoretical_answers = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79d169c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}\n",
      "{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}\n"
     ]
    }
   ],
   "source": [
    "print(predicted_answers[0])\n",
    "print(theoretical_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2a22275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b08dd69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c433a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cd04cc6710443ab83aec6709250b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0cede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29025f46ec2c433ab209b4f3e17df121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Artur\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03e1369c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFBertForQuestionAnswering' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32me:\\STUDIA\\IPS\\question-answering\\medical-qa\\notebooks\\squad_bert.ipynb Cell 45\u001B[0m line \u001B[0;36m1\n\u001B[1;32m----> <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001B[0m \u001B[39mfor\u001B[39;00m name, param \u001B[39min\u001B[39;00m model\u001B[39m.\u001B[39;49mparameters():\n\u001B[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#Y102sZmlsZQ%3D%3D?line=1'>2</a>\u001B[0m     \u001B[39mprint\u001B[39m(name, param\u001B[39m.\u001B[39mrequires_grad)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'TFBertForQuestionAnswering' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "for name, param in model.parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "193e7595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.utils.plot_model(model=model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4017687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "254a1043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_question_answering\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  107719680 \n",
      "                                                                 \n",
      " qa_outputs (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107,721,218\n",
      "Trainable params: 107,721,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70fce09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "664b2ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    train_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")\n",
    "tf_eval_dataset = model.prepare_tf_dataset(\n",
    "    validation_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c1e2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "import tensorflow as tf\n",
    "\n",
    "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
    "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
    "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
    "num_train_epochs = 3\n",
    "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9eb4ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 4989/11065 [============>.................] - ETA: 22:06 - loss: 1.4378"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32me:\\STUDIA\\IPS\\question-answering\\medical-qa\\notebooks\\squad_bert.ipynb Cell 28\u001B[0m line \u001B[0;36m4\n\u001B[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtransformers\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mkeras_callbacks\u001B[39;00m \u001B[39mimport\u001B[39;00m PushToHubCallback\n\u001B[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001B[0m \u001B[39m# We're going to do validation afterwards, so no validation mid-training\u001B[39;00m\n\u001B[1;32m----> <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001B[0m model\u001B[39m.\u001B[39;49mfit(tf_train_dataset, epochs\u001B[39m=\u001B[39;49mnum_train_epochs)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[39mreturn\u001B[39;00m fn(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mException\u001B[39;00m \u001B[39mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[39m=\u001B[39m _process_traceback_frames(e\u001B[39m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\training.py:1564\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1556\u001B[0m \u001B[39mwith\u001B[39;00m tf\u001B[39m.\u001B[39mprofiler\u001B[39m.\u001B[39mexperimental\u001B[39m.\u001B[39mTrace(\n\u001B[0;32m   1557\u001B[0m     \u001B[39m\"\u001B[39m\u001B[39mtrain\u001B[39m\u001B[39m\"\u001B[39m,\n\u001B[0;32m   1558\u001B[0m     epoch_num\u001B[39m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1561\u001B[0m     _r\u001B[39m=\u001B[39m\u001B[39m1\u001B[39m,\n\u001B[0;32m   1562\u001B[0m ):\n\u001B[0;32m   1563\u001B[0m     callbacks\u001B[39m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1564\u001B[0m     tmp_logs \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mtrain_function(iterator)\n\u001B[0;32m   1565\u001B[0m     \u001B[39mif\u001B[39;00m data_handler\u001B[39m.\u001B[39mshould_sync:\n\u001B[0;32m   1566\u001B[0m         context\u001B[39m.\u001B[39masync_wait()\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[39mreturn\u001B[39;00m fn(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mException\u001B[39;00m \u001B[39mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[39m=\u001B[39m _process_traceback_frames(e\u001B[39m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[39m=\u001B[39m \u001B[39m\"\u001B[39m\u001B[39mxla\u001B[39m\u001B[39m\"\u001B[39m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_jit_compile \u001B[39melse\u001B[39;00m \u001B[39m\"\u001B[39m\u001B[39mnonXla\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[39mwith\u001B[39;00m OptionalXlaContext(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_call(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[39m=\u001B[39m (tracing_count \u001B[39m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_lock\u001B[39m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[39m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[39m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_stateless_fn(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwds)  \u001B[39m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[39melif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_stateful_fn \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[39m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[39m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_lock\u001B[39m.\u001B[39mrelease()\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2493\u001B[0m \u001B[39mwith\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_lock:\n\u001B[0;32m   2494\u001B[0m   (graph_function,\n\u001B[0;32m   2495\u001B[0m    filtered_flat_args) \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2496\u001B[0m \u001B[39mreturn\u001B[39;00m graph_function\u001B[39m.\u001B[39;49m_call_flat(\n\u001B[0;32m   2497\u001B[0m     filtered_flat_args, captured_inputs\u001B[39m=\u001B[39;49mgraph_function\u001B[39m.\u001B[39;49mcaptured_inputs)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1858\u001B[0m possible_gradient_type \u001B[39m=\u001B[39m gradients_util\u001B[39m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1859\u001B[0m \u001B[39mif\u001B[39;00m (possible_gradient_type \u001B[39m==\u001B[39m gradients_util\u001B[39m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1860\u001B[0m     \u001B[39mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1861\u001B[0m   \u001B[39m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1862\u001B[0m   \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_build_call_outputs(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_inference_function\u001B[39m.\u001B[39;49mcall(\n\u001B[0;32m   1863\u001B[0m       ctx, args, cancellation_manager\u001B[39m=\u001B[39;49mcancellation_manager))\n\u001B[0;32m   1864\u001B[0m forward_backward \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1865\u001B[0m     args,\n\u001B[0;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1867\u001B[0m     executing_eagerly)\n\u001B[0;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[39m=\u001B[39m forward_backward\u001B[39m.\u001B[39mforward()\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[39mwith\u001B[39;00m _InterpolateFunctionError(\u001B[39mself\u001B[39m):\n\u001B[0;32m    498\u001B[0m   \u001B[39mif\u001B[39;00m cancellation_manager \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m--> 499\u001B[0m     outputs \u001B[39m=\u001B[39m execute\u001B[39m.\u001B[39;49mexecute(\n\u001B[0;32m    500\u001B[0m         \u001B[39mstr\u001B[39;49m(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49msignature\u001B[39m.\u001B[39;49mname),\n\u001B[0;32m    501\u001B[0m         num_outputs\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_num_outputs,\n\u001B[0;32m    502\u001B[0m         inputs\u001B[39m=\u001B[39;49margs,\n\u001B[0;32m    503\u001B[0m         attrs\u001B[39m=\u001B[39;49mattrs,\n\u001B[0;32m    504\u001B[0m         ctx\u001B[39m=\u001B[39;49mctx)\n\u001B[0;32m    505\u001B[0m   \u001B[39melse\u001B[39;00m:\n\u001B[0;32m    506\u001B[0m     outputs \u001B[39m=\u001B[39m execute\u001B[39m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    507\u001B[0m         \u001B[39mstr\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39msignature\u001B[39m.\u001B[39mname),\n\u001B[0;32m    508\u001B[0m         num_outputs\u001B[39m=\u001B[39m\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m         ctx\u001B[39m=\u001B[39mctx,\n\u001B[0;32m    512\u001B[0m         cancellation_manager\u001B[39m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[39m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[39m=\u001B[39m pywrap_tfe\u001B[39m.\u001B[39;49mTFE_Py_Execute(ctx\u001B[39m.\u001B[39;49m_handle, device_name, op_name,\n\u001B[0;32m     55\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[0;32m     56\u001B[0m \u001B[39mexcept\u001B[39;00m core\u001B[39m.\u001B[39m_NotOkStatusException \u001B[39mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[39mif\u001B[39;00m name \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "# We're going to do validation afterwards, so no validation mid-training\n",
    "model.fit(tf_train_dataset, epochs=num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5bedb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
